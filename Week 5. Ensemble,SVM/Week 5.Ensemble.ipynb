{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32f0c5b",
   "metadata": {},
   "source": [
    "## 앙상블 코드 리뷰 - 임지영\n",
    "\n",
    "- 원본 코드 링크 : https://www.kaggle.com/code/yassineghouzam/titanic-top-4-with-ensemble-modeling\n",
    "- 본 코드 리뷰에서는 **앙상블 모델링 과정**에 대해 집중적으로 살펴보고 정리하였다.\n",
    "- 용어를 영어로 사용하는 것이 더 편해서 영어로 리뷰한 점 참고 바랍니다.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb0bf9a",
   "metadata": {},
   "source": [
    "### Stratified K-fold \n",
    "- a type of cross validation technique used when dealing with imbalanced datasets\n",
    "- the dataset is divided into k-folds that has a similar class distribution to the original dataset\n",
    "- make sure that each fold is representitive of the overall dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5273ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10) #This code splits the dataset to 10 folds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fde850",
   "metadata": {},
   "source": [
    "### Cross validate different models \n",
    "- check scores of each classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeling step Test differents algorithms \n",
    "random_state = 2\n",
    "classifiers = []\n",
    "classifiers.append(SVC(random_state=random_state))\n",
    "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "classifiers.append(RandomForestClassifier(random_state=random_state))\n",
    "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n",
    "classifiers.append(MLPClassifier(random_state=random_state))\n",
    "classifiers.append(KNeighborsClassifier())\n",
    "classifiers.append(LogisticRegression(random_state = random_state))\n",
    "classifiers.append(LinearDiscriminantAnalysis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396878b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing score results of each classifiers\n",
    "cv_results = []\n",
    "for classifier in classifiers :\n",
    "    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n",
    "\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n",
    "\n",
    "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n",
    "g.set_xlabel(\"Mean Accuracy\")\n",
    "g = g.set_title(\"Cross validation scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b6c46",
   "metadata": {},
   "source": [
    "### Grid Search Optimization\n",
    "\n",
    " : technique used in machine learning to find the **best combinations of hyperparameters** in the model\n",
    " \n",
    " - create a grid of all possible combinations of hyperparameters\n",
    " - evaluate the model using each combination\n",
    " - effective but computationally expensive method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3d80d",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    ": a boosting algorithm that iteratively **combines weak models to create a strong model**\n",
    "\n",
    "- Weak model : simple models that performs only slightly better than random guessing\n",
    "- effective in reducing bias and variance\n",
    "- prone to overfitting if the weak learners are too complex or the data is too noisy\n",
    "\n",
    "**Process of AdaBoost**\n",
    "1. Train the weak models on different subsets of data\n",
    "2. Assign weights to the missclassified examples\n",
    "3. Train in subsequent iteration with more attention to previous misclassified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736679c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "DTC = DecisionTreeClassifier() #Use Decision tree as a weak model\n",
    "\n",
    "adaDTC = AdaBoostClassifier(DTC, random_state=7)\n",
    "\n",
    "ada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"], # use gini index, entropy to as an evaluating metric\n",
    "              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n",
    "              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n",
    "              \"n_estimators\" :[1,2],\n",
    "              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]} # many options for the learning rate\n",
    "\n",
    "gsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsadaDTC.fit(X_train,Y_train)\n",
    "\n",
    "ada_best = gsadaDTC.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6207f",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier\n",
    ": Ensemble learning method that ccombines multiple decision trees\n",
    "\n",
    "- randomly select subsets of features and thresgole values to split nodes in each tree \n",
    "- can be used both for classification and regression tasks \n",
    "- can handle high-dimensional data well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba86f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ExtraTrees \n",
    "ExtC = ExtraTreesClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "ex_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsExtC.fit(X_train,Y_train)\n",
    "\n",
    "ExtC_best = gsExtC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsExtC.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11756a1",
   "metadata": {},
   "source": [
    "### Random Forest Classifier \n",
    "\n",
    ": builds multiple decision trees and combines them to improve the model's accuracy and generalization ability\n",
    "- Each decision tree is built using a random subset of the training data and a random subset of the features\n",
    "- Random Forest is an extension of bagging, where the trees are trained independently and the final prediction is made by majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76184d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFC Parameters tunning \n",
    "RFC = RandomForestClassifier()\n",
    "\n",
    "\n",
    "## Search grid for optimal parameters\n",
    "rf_param_grid = {\"max_depth\": [None],\n",
    "              \"max_features\": [1, 3, 10],\n",
    "              \"min_samples_split\": [2, 3, 10],\n",
    "              \"min_samples_leaf\": [1, 3, 10],\n",
    "              \"bootstrap\": [False],\n",
    "              \"n_estimators\" :[100,300],\n",
    "              \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n",
    "\n",
    "gsRFC.fit(X_train,Y_train)\n",
    "\n",
    "RFC_best = gsRFC.best_estimator_\n",
    "\n",
    "# Best score\n",
    "gsRFC.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
